Data download:
from here: https://drive.google.com/drive/u/1/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M 

Extrated raw data:
    tree dbpedia_csv
    dbpedia_csv
    ├── classes.txt
    ├── readme.txt
    ├── test.csv
    └── train.csv


Data after preprocessed:
    1: data for training (to be used for getting the performance using 100% training data)
    tree dbpedia_csv
    dbpedia_csv 
    ├── classes.txt
    ├── readme.txt
    ├── test.csv
    ├── test.csv.id #tokenlized test data
    ├── train.csv
    ├── train.csv.id #tokenlized training data
    └── unigram.id #vocabulary file generated by 100% training data

    2: data for hyperparamters tuning (to be used for getting the best hyperparamters using 90% training data and 10% dev data)
    tree dbpedia_csv_for_dev
    dbpedia_csv_for_dev
    ├── dev.csv # 10% of training data
    ├── dev.csv.id # tokenlized dev data
    ├── dev_test.csv # the same data as dbpedia_csv/test.csv  
    ├── dev_test.csv.id #tokenlized dev_test data 
    ├── dev_train.csv # 90% of training data
    ├── dev_train.csv.id # tokenlized dev_train data 
    └── unigram.id #vocabulary file generated by 90% training data
